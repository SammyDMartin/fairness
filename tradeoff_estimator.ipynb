{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd08fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb",
   "display_name": "Python 3.7.6 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "                import load_preproc_data_adult, load_preproc_data_compas\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.model_selection import KFold as cross_val_split\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load custom functions\n",
    "from variable_cep import VariableCEP as CalibratedEqOddsPostprocessing #modified for varying weight\n",
    "from variable_cep import InquisitiveRejectOptionClassification\n",
    "from variable_cep import normed_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset - use compas\n",
    "dataset_used = \"compas\" # \"adult\", \"german\", \"compas\"\n",
    "protected_attribute_used = 2 # 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to identify the protected attributes from all of the dataset features\n",
    "if dataset_used == \"adult\":\n",
    "    dataset_orig = AdultDataset()\n",
    "#     dataset_orig = load_preproc_data_adult()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "    \n",
    "elif dataset_used == \"german\":\n",
    "    dataset_orig = GermanDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "    \n",
    "elif dataset_used == \"compas\":\n",
    "#     dataset_orig = CompasDataset()\n",
    "    dataset_orig = load_preproc_data_compas()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed for calibrated equal odds prediction\n",
    "randseed = 12345679 \n",
    "\n",
    "#train validation&test split\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.6], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for predicted and transformed datasets\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "\n",
    "# Logistic regression classifier and predictions for training data\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "lmod = LogisticRegression() #logregression\n",
    "\n",
    "#fit original model\n",
    "lmod.fit(X_train, y_train)\n",
    "\n",
    "fav_idx = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "y_train_pred_prob = lmod.predict_proba(X_train)[:,fav_idx]\n",
    "\n",
    "# Prediction probs for training data\n",
    "class_thresh = 0.5\n",
    "dataset_orig_train_pred.scores = y_train_pred_prob.reshape(-1,1)\n",
    "\n",
    "y_train_pred = np.zeros_like(dataset_orig_train_pred.labels)\n",
    "y_train_pred[y_train_pred_prob >= class_thresh] = dataset_orig_train_pred.favorable_label\n",
    "y_train_pred[~(y_train_pred_prob >= class_thresh)] = dataset_orig_train_pred.unfavorable_label\n",
    "dataset_orig_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set up tradeoff cost-benefit calculation\n",
    "include = False\n",
    "N_reps = 10\n",
    "N_values = 50\n",
    "\n",
    "\n",
    "negs = []\n",
    "accs = []\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "privileged_options = [True,False,None]\n",
    "\n",
    "\n",
    "#whether to include all fp and all fn cost functions (True)\n",
    "if include == True:\n",
    "    n_range = np.linspace(0.00,1.00,N_values)\n",
    "if include == False:\n",
    "    n_range = np.linspace(0.01,0.99,N_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up equalized odds processing\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                        unprivileged_groups = unprivileged_groups,\n",
    "                                        seed=randseed)\n",
    "#ROC\n",
    "ROC = InquisitiveRejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  # metric_name=metric_name,\n",
    "                                  metric_ub=0.05, metric_lb=-0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for if you're using ROC - which f1, f2 to use for output results AND optimization\n",
    "f1_name = 'false_negative_rate'\n",
    "f2_name = 'false_positive_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable to use Mojo's ROC optimiser\n",
    "use_cpp = True\n",
    "split_interv = 0.8\n",
    "\n",
    "#scale size of input range - are these normalized??\n",
    "scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cpp == True:\n",
    "    post_processor = cpp\n",
    "elif use_cpp == False:\n",
    "    post_processor = ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vt_split_process(dataset_orig_vt,split_indexes,shuf,class_thresh):\n",
    "    dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split(num_or_size_splits=split_indexes,shuffle=shuf,seed=randseed)#validation_test split)\n",
    "    dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "    dataset_new_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_new_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "    X_valid = scale_orig.transform(dataset_orig_valid.features)\n",
    "    y_valid_pred_prob = lmod.predict_proba(X_valid)[:,fav_idx]\n",
    "\n",
    "    X_test = scale_orig.transform(dataset_orig_test.features)\n",
    "    y_test_pred_prob = lmod.predict_proba(X_test)[:,fav_idx]\n",
    "\n",
    "    dataset_orig_valid_pred.scores = y_valid_pred_prob.reshape(-1,1)\n",
    "    dataset_orig_test_pred.scores = y_test_pred_prob.reshape(-1,1)\n",
    "\n",
    "    y_valid_pred = np.zeros_like(dataset_orig_valid_pred.labels)\n",
    "    y_valid_pred[y_valid_pred_prob >= class_thresh] = dataset_orig_valid_pred.favorable_label\n",
    "    y_valid_pred[~(y_valid_pred_prob >= class_thresh)] = dataset_orig_valid_pred.unfavorable_label\n",
    "    dataset_orig_valid_pred.labels = y_valid_pred\n",
    "        \n",
    "    y_test_pred = np.zeros_like(dataset_orig_test_pred.labels)\n",
    "    y_test_pred[y_test_pred_prob >= class_thresh] = dataset_orig_test_pred.favorable_label\n",
    "    y_test_pred[~(y_test_pred_prob >= class_thresh)] = dataset_orig_test_pred.unfavorable_label\n",
    "    dataset_orig_test_pred.labels = y_test_pred\n",
    "\n",
    "    return (dataset_orig_valid, dataset_orig_valid_pred, dataset_orig_test, dataset_orig_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_cv_values(folds,post_processor,weight_tuple,name_tuple,pbar=None):\n",
    "    for stream in folds:\n",
    "        # Odds equalizing post-processing algorithm\n",
    "\n",
    "        m1,m2,acc = np.zeros(3),np.zeros(3),np.zeros(3)\n",
    "\n",
    "        ##########\n",
    "        dataset_orig_valid, dataset_orig_valid_pred, dataset_orig_test, dataset_orig_test_pred = stream\n",
    "\n",
    "        # Learn parameters to equalize odds and apply to create a new dataset\n",
    "   \n",
    "        if type(post_processor) == CalibratedEqOddsPostprocessing:\n",
    "            f1_name = 'false_negative_rate'\n",
    "            f2_name = 'false_positive_rate'\n",
    "            post_processor.set_NP(weight_tuple)\n",
    "            post_processor.fit(dataset_orig_valid, dataset_orig_valid_pred)\n",
    "        elif type(post_processor) == InquisitiveRejectOptionClassification:\n",
    "            f1_name = name_tuple[0]\n",
    "            f2_name = name_tuple[1]\n",
    "\n",
    "            post_processor.fit(dataset_orig_valid, dataset_orig_valid_pred, metric_fn = lambda metric: \n",
    "                            weight_tuple[0]*getattr(metric,f1_name)() + weight_tuple[1]*getattr(metric,f2_name)())\n",
    "\n",
    "        dataset_transf_test_pred = post_processor.predict(dataset_orig_test_pred)\n",
    "\n",
    "        \"\"\"\n",
    "        dataset_transf_valid_pred = post_processor.predict(dataset_orig_valid_pred)\n",
    "        cm_transf_valid = ClassificationMetric(dataset_orig_valid, dataset_transf_valid_pred,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "        \"\"\"\n",
    "        cm_transf_test = ClassificationMetric(dataset_orig_test, dataset_transf_test_pred,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "        #cm_transf_test.difference\n",
    "        \n",
    "        for idx,PR in enumerate(privileged_options):\n",
    "            try:\n",
    "                m1[idx] += getattr(cm_transf_test,f1_name)(privileged=PR)\n",
    "            except Exception:\n",
    "                #for if there is no difference for privileged/unpriveleged groups due to nature of the metric\n",
    "                m1[idx] = 0\n",
    "            try:\n",
    "                m2[idx] += getattr(cm_transf_test,f2_name)(privileged=PR)\n",
    "            except Exception:\n",
    "                m2[idx] = 0\n",
    "            result = cm_transf_test.accuracy(privileged=PR)\n",
    "            acc[idx] += float(result)\n",
    "\n",
    "        out = float(m2[1]/len(folds))\n",
    "        if diag is True:\n",
    "            print(weight_tuple, out)\n",
    "        \n",
    "        if pbar is not None:\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            return out\n",
    "    return m1/len(folds),m2/len(folds),acc/len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold = cross_val_split(n_splits=reruns)\n",
    "\n",
    "folds = []\n",
    "\n",
    "for count in tqdm(range(N_reps)):\n",
    "    si = [split_interv]\n",
    "    folds.append(vt_split_process(dataset_orig_vt=dataset_orig_vt,split_indexes=si,shuf=True,class_thresh=class_thresh))\n",
    "\n",
    "#valid, test = dataset_orig_vt.split(num_or_size_splits=si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_orig_valid_pred,dataset_orig_test_pred, dataset_new_valid_pred, dataset_new_test_pred = None,None,None,None\n",
    "pbar = tqdm(total=(N_reps*N_values))\n",
    "for neg in n_range:\n",
    "    f1_weight = neg\n",
    "\n",
    "    f2_weight = scale*(1.0 - f1_weight)\n",
    "    f1_weight = scale*f1_weight\n",
    "\n",
    "    w1_score,w2_score,accuracy = determine_cv_values(folds=folds,post_processor=post_processor,weight_tuple=(f1_weight,f2_weight),name_tuple=(f1_name,f2_name),pbar=pbar)\n",
    "\n",
    "    fns.append(w1_score)\n",
    "    fps.append(w2_score)\n",
    "    accs.append(accuracy)\n",
    "    negs.append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results from fns, fps, accs, negs for different groups\n",
    "collapse = lambda param, idx : [v[idx] for v in param]\n",
    "\n",
    "getnames = {None:\"full data\", True:\"privileged\", False:\"unprivileged\"}\n",
    "\n",
    "percentchange = lambda result: [100*((i-result[0])/result[0]) for i in result]\n",
    "\n",
    "idx,PR = 2,None\n",
    "maximised_accuracy = max(collapse(accs,idx))\n",
    "maximised_idx = collapse(accs,idx).index(maximised_accuracy)\n",
    "percentmax = lambda result: [100*((i-result[maximised_idx])/result[maximised_idx]) for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximised_value_idx = lambda prop, idx : collapse(prop,idx).index(max(collapse(prop,idx)))\n",
    "idx = 2\n",
    "values = ['min {}'.format(f1_name),'min {}'.format(f2_name), 'max accuracy']\n",
    "for idv,prop in enumerate([-1*np.array(fps),-1*np.array(fns),accs]):\n",
    "    print(\"\\n\"+values[idv])\n",
    "    maxi = maximised_value_idx(prop,idx)\n",
    "\n",
    "\n",
    "    print(\"{}: {}\".format(f1_name, round(collapse(fps,idx)[maxi],5)))\n",
    "    print(\"{}: {}\".format(f2_name, round(collapse(fns,idx)[maxi],5)))\n",
    "    print(\"accs: {}\".format(round(collapse(accs,idx)[maxi],5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show accuracy, fp,fn % change from max accuracy for whole model\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(negs,percentmax(collapse(accs,idx)),label='cross validation accuracy')\n",
    "plt.plot(negs,percentmax(collapse(fns,idx)),label='cross validation {}'.format(f1_name))\n",
    "plt.plot(negs,percentmax(collapse(fps,idx)),label='cross validation {}'.format(f2_name))\n",
    "plt.xlabel('fn weight in cost fn')\n",
    "plt.ylabel('percent change from max value')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('compas1d {}\\nmax_acc={}, fn_cost={}'.format(getnames[PR],round(maximised_accuracy,4),round(negs[maximised_idx],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show accuracy, fp,fn values % change around the max accuracy for whole model\n",
    "width = 50\n",
    "if maximised_idx < width:\n",
    "    minv,maxv = 0,2*maximised_idx\n",
    "else:\n",
    "    minv,maxv = maximised_idx-width,maximised_idx+width\n",
    "\n",
    "idx,PR = 2,None\n",
    "\n",
    "diceup = lambda lst : lst[minv:maxv]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(diceup(negs),diceup(percentmax(collapse(accs,idx))),label='cross validation accuracy')\n",
    "plt.plot(diceup(negs),diceup(percentmax(collapse(fns,idx))),label='cross validation {}'.format(f1_name))\n",
    "plt.plot(diceup(negs),diceup(percentmax(collapse(fps,idx))),label='cross validation {}'.format(f2_name))\n",
    "plt.xlabel('fn weight in cost fn')\n",
    "plt.ylabel('percent change from max value')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('compas1d {}\\nmax_acc={}, fn_cost={}'.format(getnames[PR],round(maximised_accuracy,4),round(negs[maximised_idx],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show accuracy, fp,fn % change from the start\n",
    "for idx,PR in enumerate(privileged_options):\n",
    "    cost_0 = negs[0]\n",
    "    plt.figure()\n",
    "    plt.plot(negs,percentchange(collapse(accs,idx)),label='cross validation accuracy')\n",
    "    plt.plot(negs,percentchange(collapse(fns,idx)),label='cross validation {}'.format(f1_name))\n",
    "    plt.plot(negs,percentchange(collapse(fps,idx)),label='cross validation {}'.format(f2_name))\n",
    "    plt.xlabel('fn weight in cost fn')\n",
    "    plt.ylabel('percent change from start value')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('compas1d {}\\npercent change over fn_cost={}'.format(getnames[PR],cost_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show raw accuracy,fp,fn\n",
    "for idx,PR in enumerate(privileged_options):\n",
    "    plt.figure()\n",
    "    plt.plot(negs,collapse(accs,idx),label='accuracy')\n",
    "    plt.plot(negs,collapse(fns,idx),label='{}'.format(f1_name))\n",
    "    plt.plot(negs,collapse(fps,idx),label='{}'.format(f2_name))\n",
    "    plt.xlabel('unnormalized fn rate cost')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('compas1d {}'.format(getnames[PR]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(negs,collapse(accs,idx),label='accuracy')\n",
    "    plt.xlabel('unnormalized fn rate cost')\n",
    "    plt.legend()\n",
    "    plt.title('compas1d_acc {}'.format(getnames[PR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#OPTIMIZE SEARCH\n",
    "diag = True\n",
    "process = lambda weight : [weight, 1-weight]\n",
    "to_opt = lambda weights : -1*determine_cv_values(folds=folds,post_processor=post_processor,weight_tuple=process(weights[0]),name_tuple=(f1_name,f2_name))\n",
    "res = optimize.basinhopping(func=to_opt,x0 = np.array([0.3]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}